{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nitastha/Desktop/NitishFiles/Work/Optum/project\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 11:46:55,950 - INFO - Processing field 1/10: Rx Bc Demographics.Rx BC Email\n",
      "2025-02-17 11:47:03,573 - ERROR - JSON parsing error: Invalid \\escape: line 54 column 37 (char 1503)\n",
      "2025-02-17 11:47:03,574 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 11:47:09,708 - INFO - Successfully generated 14 test cases\n",
      "2025-02-17 11:47:09,709 - INFO - Processing field 2/10: Rx Bc Demographics.Rx BC First Name\n",
      "2025-02-17 11:47:14,191 - ERROR - JSON parsing error: Expecting ',' delimiter: line 66 column 20 (char 1770)\n",
      "2025-02-17 11:47:14,192 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 11:47:18,208 - INFO - Successfully generated 10 test cases\n",
      "2025-02-17 11:47:18,208 - INFO - Processing field 3/10: Rx Bc Demographics.Rx BC Last Name\n",
      "2025-02-17 11:47:23,415 - INFO - Successfully generated 13 test cases\n",
      "2025-02-17 11:47:23,416 - INFO - Processing field 4/10: Rx BC Email Event.Rx BC Email\n",
      "2025-02-17 11:47:29,061 - ERROR - JSON parsing error: Invalid \\escape: line 54 column 37 (char 1530)\n",
      "2025-02-17 11:47:29,062 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 11:47:34,386 - ERROR - JSON parsing error: Invalid \\escape: line 48 column 37 (char 1339)\n",
      "2025-02-17 11:47:34,388 - WARNING - Attempt 2: Failed to generate valid test cases\n",
      "2025-02-17 11:47:39,954 - ERROR - JSON parsing error: Invalid \\escape: line 48 column 37 (char 1345)\n",
      "2025-02-17 11:47:39,955 - WARNING - Attempt 3: Failed to generate valid test cases\n",
      "2025-02-17 11:47:39,955 - INFO - Processing field 5/10: Rx BC Email Event.Rx BC Email Template Info\n",
      "2025-02-17 11:47:44,681 - ERROR - JSON parsing error: Invalid \\escape: line 36 column 37 (char 1117)\n",
      "2025-02-17 11:47:44,682 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 11:47:49,720 - ERROR - JSON parsing error: Invalid \\escape: line 18 column 89 (char 744)\n",
      "2025-02-17 11:47:49,721 - WARNING - Attempt 2: Failed to generate valid test cases\n",
      "2025-02-17 11:47:55,271 - INFO - Successfully generated 11 test cases\n",
      "2025-02-17 11:47:55,272 - INFO - Processing field 6/10: Rx BC Email Event.Rx BC Event ID\n",
      "2025-02-17 11:48:00,294 - INFO - Successfully generated 12 test cases\n",
      "2025-02-17 11:48:00,295 - INFO - Processing field 7/10: Rx BC Email Event.Rx BC Event Type\n",
      "2025-02-17 11:48:05,306 - INFO - Successfully generated 12 test cases\n",
      "2025-02-17 11:48:05,307 - INFO - Processing field 8/10: Rx BC Email Event.Rx BC New User Registration Link\n",
      "2025-02-17 11:48:10,840 - ERROR - JSON parsing error: Invalid \\escape: line 42 column 61 (char 1307)\n",
      "2025-02-17 11:48:10,841 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 11:48:16,371 - INFO - Successfully generated 10 test cases\n",
      "2025-02-17 11:48:16,372 - INFO - Processing field 9/10: Rx BC Email Event.Rx BC Timestamp\n",
      "2025-02-17 11:48:23,436 - INFO - Successfully generated 14 test cases\n",
      "2025-02-17 11:48:23,437 - INFO - Processing field 10/10: Rx BC Email Event.Rx BC User Group\n",
      "2025-02-17 11:48:27,942 - INFO - Successfully generated 11 test cases\n",
      "2025-02-17 11:48:27,945 - INFO - Successfully saved test cases to data/anth.json\n",
      "2025-02-17 11:48:27,946 - INFO - \n",
      "Test Case Generation Summary\n",
      "==============================\n",
      "Total fields processed: 9\n",
      "Total test cases generated: 107\n",
      "Average test cases per field: 11.89\n",
      "Output file: data/anth.json\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional, Any\n",
    "import google.generativeai as genai\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('test_generation.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self, config_path: str = \"config/settings.yaml\"):\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.llm_client = self._initialize_llm()\n",
    "        \n",
    "    def _load_config(self, config_path: str) -> dict:\n",
    "        \"\"\"Load configuration from YAML file with error handling.\"\"\"\n",
    "        try:\n",
    "            with open(config_path, \"r\") as f:\n",
    "                return yaml.safe_load(f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load config: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_llm(self) -> genai.GenerativeModel:\n",
    "        \"\"\"Initialize the LLM client with error handling.\"\"\"\n",
    "        try:\n",
    "            if not self.config.get(\"gemini_api_key\"):\n",
    "                raise ValueError(\"Gemini API key not found in config\")\n",
    "            \n",
    "            genai.configure(api_key=self.config[\"gemini_api_key\"])\n",
    "            model_name = self.config.get(\"gemini_model\", \"gemini-1.5-flash\")\n",
    "            return genai.GenerativeModel(model_name)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize LLM: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _generate_prompt(self, field_name: str, data_type: str, constraints: List[str]) -> str:\n",
    "        \"\"\"Generate a more structured and specific prompt for test case generation.\"\"\"\n",
    "        return f\"\"\"\n",
    "Generate test cases for the field '{field_name}' with following specifications:\n",
    "- Data Type: {data_type}\n",
    "- Constraints: {constraints}\n",
    "\n",
    "Requirements:\n",
    "1. Include ONLY the JSON array of test cases in your response\n",
    "2. Each test case must have these exact fields:\n",
    "   - \"test_case\": A clear, unique identifier for the test\n",
    "   - \"description\": Detailed explanation of what the test verifies\n",
    "   - \"expected_result\": MUST be exactly \"Pass\" or \"Fail\"\n",
    "   - \"input\": The test input value (can be null, string, number, etc.)\n",
    "\n",
    "3. Include these types of test cases:\n",
    "   - Basic valid inputs\n",
    "   - Basic invalid inputs\n",
    "   - Null/empty handling\n",
    "   - Boundary conditions\n",
    "   - Edge cases\n",
    "   - Type validation\n",
    "\n",
    "Return the response in this exact format:\n",
    "[\n",
    "    {{\n",
    "        \"test_case\": \"TC001_Valid_Basic\",\n",
    "        \"description\": \"Basic valid input test\",\n",
    "        \"expected_result\": \"Pass\",\n",
    "        \"input\": \"example\"\n",
    "    }},\n",
    "    {{\n",
    "        \"test_case\": \"TC002_Invalid_Null\",\n",
    "        \"description\": \"Test with null input\",\n",
    "        \"expected_result\": \"Fail\",\n",
    "        \"input\": null\n",
    "    }}\n",
    "]\n",
    "\n",
    "IMPORTANT: Return ONLY the JSON array. No additional text or explanation.\"\"\"\n",
    "\n",
    "    def _parse_llm_response(self, response_text: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        \"\"\"Parse and validate LLM response with improved error handling.\"\"\"\n",
    "        try:\n",
    "            # Remove any markdown code blocks if present\n",
    "            cleaned_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            test_cases = json.loads(cleaned_text)\n",
    "            \n",
    "            # Validate structure\n",
    "            if not isinstance(test_cases, list):\n",
    "                raise ValueError(\"Response is not a JSON array\")\n",
    "            \n",
    "            # Validate and normalize each test case\n",
    "            validated_cases = []\n",
    "            for idx, case in enumerate(test_cases, 1):\n",
    "                required_fields = {\"test_case\", \"description\", \"expected_result\", \"input\"}\n",
    "                if not all(field in case for field in required_fields):\n",
    "                    logging.warning(f\"Test case {idx} missing required fields, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Normalize expected_result to Pass/Fail\n",
    "                case[\"expected_result\"] = \"Pass\" if case[\"expected_result\"].lower() == \"pass\" else \"Fail\"\n",
    "                validated_cases.append(case)\n",
    "            \n",
    "            return validated_cases\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"JSON parsing error: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error parsing response: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def generate_test_cases(self, rules_file: str, output_file: str) -> None:\n",
    "        \"\"\"Main method to generate and save test cases.\"\"\"\n",
    "        try:\n",
    "            # Load rules\n",
    "            with open(rules_file, \"r\") as f:\n",
    "                rules = json.load(f)\n",
    "\n",
    "            all_test_cases = {}\n",
    "            total_fields = sum(len(details[\"fields\"]) for details in rules.values())\n",
    "            processed_fields = 0\n",
    "\n",
    "            # Process each field\n",
    "            for parent_field, details in rules.items():\n",
    "                for field_name, field_details in details[\"fields\"].items():\n",
    "                    full_field_name = f\"{parent_field}.{field_name}\"\n",
    "                    logging.info(f\"Processing field {processed_fields + 1}/{total_fields}: {full_field_name}\")\n",
    "\n",
    "                    # Generate prompt\n",
    "                    prompt = self._generate_prompt(\n",
    "                        field_name,\n",
    "                        field_details[\"data_type\"],\n",
    "                        field_details[\"constraints\"]\n",
    "                    )\n",
    "\n",
    "                    # Get LLM response with retries\n",
    "                    max_retries = 3\n",
    "                    for attempt in range(max_retries):\n",
    "                        try:\n",
    "                            response = self.llm_client.generate_content(prompt)\n",
    "                            test_cases = self._parse_llm_response(response.text)\n",
    "                            \n",
    "                            if test_cases:\n",
    "                                all_test_cases[full_field_name] = test_cases\n",
    "                                logging.info(f\"Successfully generated {len(test_cases)} test cases\")\n",
    "                                break\n",
    "                            else:\n",
    "                                logging.warning(f\"Attempt {attempt + 1}: Failed to generate valid test cases\")\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                            if attempt == max_retries - 1:\n",
    "                                logging.error(f\"Failed to generate test cases for {full_field_name} after {max_retries} attempts\")\n",
    "                    \n",
    "                    processed_fields += 1\n",
    "\n",
    "            # Save results\n",
    "            self._save_test_cases(all_test_cases, output_file)\n",
    "            \n",
    "            # Generate summary\n",
    "            self._generate_summary(all_test_cases, output_file)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate test cases: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _save_test_cases(self, test_cases: Dict[str, List[Dict[str, Any]]], output_file: str) -> None:\n",
    "        \"\"\"Save test cases with backup.\"\"\"\n",
    "        try:\n",
    "            # Create backup of existing file if it exists\n",
    "            if os.path.exists(output_file):\n",
    "                backup_file = f\"{output_file}.{datetime.now().strftime('%Y%m%d_%H%M%S')}.bak\"\n",
    "                os.rename(output_file, backup_file)\n",
    "                logging.info(f\"Created backup: {backup_file}\")\n",
    "\n",
    "            # Save new test cases\n",
    "            with open(output_file, \"w\") as f:\n",
    "                json.dump(test_cases, f, indent=2)\n",
    "            logging.info(f\"Successfully saved test cases to {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save test cases: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _generate_summary(self, test_cases: Dict[str, List[Dict[str, Any]]], output_file: str) -> None:\n",
    "        \"\"\"Generate a summary of the test case generation.\"\"\"\n",
    "        total_fields = len(test_cases)\n",
    "        total_test_cases = sum(len(cases) for cases in test_cases.values())\n",
    "        \n",
    "        summary = (\n",
    "            f\"\\nTest Case Generation Summary\\n\"\n",
    "            f\"{'='*30}\\n\"\n",
    "            f\"Total fields processed: {total_fields}\\n\"\n",
    "            f\"Total test cases generated: {total_test_cases}\\n\"\n",
    "            f\"Average test cases per field: {total_test_cases/total_fields:.2f}\\n\"\n",
    "            f\"Output file: {output_file}\\n\"\n",
    "            f\"{'='*30}\"\n",
    "        )\n",
    "        \n",
    "        logging.info(summary)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        generator = TestCaseGenerator()\n",
    "        generator.generate_test_cases(\n",
    "            generator.config[\"constrains_processed_rules_file\"],\n",
    "            generator.config[\"generated_test_cases_file\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Application failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 15:21:59,719 - INFO - Processing field 1/10: Rx Bc Demographics.Rx BC Email\n",
      "2025-02-17 15:22:07,080 - ERROR - JSON parsing error: Invalid \\escape: line 54 column 37 (char 1680)\n",
      "2025-02-17 15:22:07,081 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 15:22:12,612 - INFO - Successfully generated 13 test cases\n",
      "2025-02-17 15:22:12,614 - INFO - Processing field 2/10: Rx Bc Demographics.Rx BC First Name\n",
      "2025-02-17 15:22:17,320 - INFO - Successfully generated 12 test cases\n",
      "2025-02-17 15:22:17,321 - INFO - Processing field 3/10: Rx Bc Demographics.Rx BC Last Name\n",
      "2025-02-17 15:22:22,589 - INFO - Successfully generated 13 test cases\n",
      "2025-02-17 15:22:22,590 - INFO - Processing field 4/10: Rx BC Email Event.Rx BC Email\n",
      "2025-02-17 15:22:28,685 - ERROR - JSON parsing error: Invalid \\escape: line 48 column 37 (char 1329)\n",
      "2025-02-17 15:22:28,686 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 15:22:33,499 - INFO - Successfully generated 11 test cases\n",
      "2025-02-17 15:22:33,500 - INFO - Processing field 5/10: Rx BC Email Event.Rx BC Email Template Info\n",
      "2025-02-17 15:22:37,491 - ERROR - JSON parsing error: Invalid \\escape: line 18 column 58 (char 658)\n",
      "2025-02-17 15:22:37,493 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 15:22:42,193 - ERROR - JSON parsing error: Invalid \\escape: line 18 column 58 (char 655)\n",
      "2025-02-17 15:22:42,194 - WARNING - Attempt 2: Failed to generate valid test cases\n",
      "2025-02-17 15:22:46,428 - ERROR - JSON parsing error: Invalid \\escape: line 18 column 58 (char 654)\n",
      "2025-02-17 15:22:46,428 - WARNING - Attempt 3: Failed to generate valid test cases\n",
      "2025-02-17 15:22:46,429 - INFO - Processing field 6/10: Rx BC Email Event.Rx BC Event ID\n",
      "2025-02-17 15:22:51,167 - INFO - Successfully generated 11 test cases\n",
      "2025-02-17 15:22:51,168 - INFO - Processing field 7/10: Rx BC Email Event.Rx BC Event Type\n",
      "2025-02-17 15:22:55,003 - INFO - Successfully generated 9 test cases\n",
      "2025-02-17 15:22:55,004 - INFO - Processing field 8/10: Rx BC Email Event.Rx BC New User Registration Link\n",
      "2025-02-17 15:22:59,921 - INFO - Successfully generated 11 test cases\n",
      "2025-02-17 15:22:59,923 - INFO - Processing field 9/10: Rx BC Email Event.Rx BC Timestamp\n",
      "2025-02-17 15:23:07,332 - WARNING - Test case 4 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:23:07,333 - WARNING - Test case 5 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:23:07,334 - WARNING - Test case 7 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:23:07,335 - WARNING - Test case 12 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:23:07,336 - WARNING - Test case 13 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:23:07,337 - WARNING - Test case 14 validation failed: Date input must be a string\n",
      "2025-02-17 15:23:07,337 - WARNING - Test case 15 validation failed: Date input must be a string\n",
      "2025-02-17 15:23:07,338 - INFO - Successfully generated 8 test cases\n",
      "2025-02-17 15:23:07,339 - INFO - Processing field 10/10: Rx BC Email Event.Rx BC User Group\n",
      "2025-02-17 15:23:11,591 - INFO - Successfully generated 10 test cases\n",
      "2025-02-17 15:23:11,595 - INFO - Successfully saved test cases to data/anth1.json\n",
      "2025-02-17 15:23:11,596 - INFO - \n",
      "Test Case Generation Summary\n",
      "==============================\n",
      "Total fields processed: 9\n",
      "Total test cases generated: 98\n",
      "Average test cases per field: 10.89\n",
      "Output file: data/anth1.json\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "import google.generativeai as genai\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('test_generation.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self, config_path: str = \"config/settings.yaml\"):\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.llm_client = self._initialize_llm()\n",
    "        self.field_specific_rules = self._initialize_field_rules()\n",
    "        \n",
    "    def _load_config(self, config_path: str) -> dict:\n",
    "        \"\"\"Load configuration from YAML file with error handling.\"\"\"\n",
    "        try:\n",
    "            with open(config_path, \"r\") as f:\n",
    "                return yaml.safe_load(f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load config: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_llm(self) -> genai.GenerativeModel:\n",
    "        \"\"\"Initialize the LLM client with error handling.\"\"\"\n",
    "        try:\n",
    "            if not self.config.get(\"gemini_api_key\"):\n",
    "                raise ValueError(\"Gemini API key not found in config\")\n",
    "            \n",
    "            genai.configure(api_key=self.config[\"gemini_api_key\"])\n",
    "            model_name = self.config.get(\"gemini_model\", \"gemini-1.5-flash\")\n",
    "            return genai.GenerativeModel(model_name)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize LLM: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_field_rules(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Initialize specific rules for different field types.\"\"\"\n",
    "        return {\n",
    "            \"Date\": {\n",
    "                \"valid_formats\": [\n",
    "                    \"%Y-%m-%d %H:%M:%S\",\n",
    "                    \"%Y/%m/%d %H:%M:%S\",\n",
    "                    \"%m/%d/%Y %H:%M:%S\"\n",
    "                ],\n",
    "                \"extra_validation\": self._validate_date_format\n",
    "            },\n",
    "            \"String\": {\n",
    "                \"extra_validation\": self._validate_string_format\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _validate_date_format(self, test_case: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate date format test cases.\"\"\"\n",
    "        if test_case[\"input\"] is None:\n",
    "            return True, \"\"\n",
    "            \n",
    "        if isinstance(test_case[\"input\"], str):\n",
    "            for date_format in self.field_specific_rules[\"Date\"][\"valid_formats\"]:\n",
    "                try:\n",
    "                    datetime.strptime(test_case[\"input\"], date_format)\n",
    "                    return True, \"\"\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            return False, f\"Invalid date format. Expected formats: {self.field_specific_rules['Date']['valid_formats']}\"\n",
    "        return False, \"Date input must be a string\"\n",
    "\n",
    "    def _validate_string_format(self, test_case: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate string format test cases.\"\"\"\n",
    "        if test_case[\"input\"] is None:\n",
    "            return True, \"\"\n",
    "            \n",
    "        if not isinstance(test_case[\"input\"], (str, type(None))):\n",
    "            if test_case[\"expected_result\"] == \"Pass\":\n",
    "                return False, \"String field with non-string input should fail\"\n",
    "        return True, \"\"\n",
    "\n",
    "    def _generate_prompt(self, field_name: str, data_type: str, constraints: List[str], description: str = \"\") -> str:\n",
    "        \"\"\"Generate a more structured and specific prompt for test case generation.\"\"\"\n",
    "        field_specific_info = \"\"\n",
    "        if data_type == \"Date\":\n",
    "            field_specific_info = \"\\nFor Date fields, use these formats only:\\n\" + \\\n",
    "                                \"\\n\".join(f\"- {fmt}\" for fmt in self.field_specific_rules[\"Date\"][\"valid_formats\"])\n",
    "\n",
    "        return f\"\"\"\n",
    "Generate test cases for the field '{field_name}' with following specifications:\n",
    "- Data Type: {data_type}\n",
    "- Constraints: {constraints}\n",
    "- Description: {description}{field_specific_info}\n",
    "\n",
    "Requirements:\n",
    "1. Include ONLY the JSON array of test cases in your response\n",
    "2. Each test case must have these exact fields:\n",
    "   - \"test_case\": A clear, unique identifier for the test\n",
    "   - \"description\": Detailed explanation of what the test verifies\n",
    "   - \"expected_result\": MUST be exactly \"Pass\" or \"Fail\"\n",
    "   - \"input\": The test input value (can be null, string, number, etc.)\n",
    "\n",
    "3. Include these types of test cases:\n",
    "   - Basic valid inputs\n",
    "   - Basic invalid inputs\n",
    "   - Null/empty handling\n",
    "   - Boundary conditions\n",
    "   - Edge cases\n",
    "   - Type validation\n",
    "\n",
    "4. Consider field-specific requirements:\n",
    "   - For Date fields: Include only valid date formats specified\n",
    "   - For String fields: Consider length limits and character restrictions\n",
    "   - Handle nullable fields appropriately based on constraints\n",
    "\n",
    "Return the response in this exact format:\n",
    "[\n",
    "    {{\n",
    "        \"test_case\": \"TC001_Valid_Basic\",\n",
    "        \"description\": \"Basic valid input test\",\n",
    "        \"expected_result\": \"Pass\",\n",
    "        \"input\": \"example\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "IMPORTANT: Return ONLY the JSON array. No additional text or explanation.\"\"\"\n",
    "\n",
    "    def _validate_test_case(self, test_case: Dict[str, Any], data_type: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate a single test case based on field type and rules.\"\"\"\n",
    "        if not all(field in test_case for field in [\"test_case\", \"description\", \"expected_result\", \"input\"]):\n",
    "            return False, \"Missing required fields\"\n",
    "\n",
    "        if test_case[\"expected_result\"] not in [\"Pass\", \"Fail\"]:\n",
    "            return False, \"Invalid expected_result value\"\n",
    "\n",
    "        # Apply field-specific validation\n",
    "        if data_type in self.field_specific_rules:\n",
    "            return self.field_specific_rules[data_type][\"extra_validation\"](test_case)\n",
    "\n",
    "        return True, \"\"\n",
    "\n",
    "    def _parse_llm_response(self, response_text: str, data_type: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        \"\"\"Parse and validate LLM response with improved error handling.\"\"\"\n",
    "        try:\n",
    "            # Remove Markdown JSON blocks if present\n",
    "            cleaned_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "            # Handle invalid escape sequences\n",
    "            cleaned_text = re.sub(r'\\\\([^\"\\\\])', r'\\\\\\\\\\1', cleaned_text)  \n",
    "\n",
    "            # Parse JSON\n",
    "            test_cases = json.loads(cleaned_text)\n",
    "\n",
    "            # Validate structure\n",
    "            if not isinstance(test_cases, list):\n",
    "                raise ValueError(\"Response is not a JSON array\")\n",
    "\n",
    "            # Validate and normalize each test case\n",
    "            validated_cases = []\n",
    "            for idx, case in enumerate(test_cases, 1):\n",
    "                is_valid, error_msg = self._validate_test_case(case, data_type)\n",
    "                if not is_valid:\n",
    "                    logging.warning(f\"Test case {idx} validation failed: {error_msg}\")\n",
    "                    continue\n",
    "                \n",
    "                # Normalize expected_result to Pass/Fail\n",
    "                case[\"expected_result\"] = \"Pass\" if case[\"expected_result\"].lower() == \"pass\" else \"Fail\"\n",
    "                validated_cases.append(case)\n",
    "\n",
    "            return validated_cases\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"JSON parsing error: {str(e)} - Raw response: {response_text}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error parsing response: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def generate_test_cases(self, rules_file: str, output_file: str) -> None:\n",
    "        \"\"\"Main method to generate and save test cases.\"\"\"\n",
    "        try:\n",
    "            # Load rules\n",
    "            with open(rules_file, \"r\") as f:\n",
    "                rules = json.load(f)\n",
    "\n",
    "            all_test_cases = {}\n",
    "            total_fields = sum(len(details[\"fields\"]) for details in rules.values())\n",
    "            processed_fields = 0\n",
    "\n",
    "\n",
    "            for parent_field, details in rules.items():\n",
    "                for field_name, field_details in details[\"fields\"].items():\n",
    "                    full_field_name = f\"{parent_field}.{field_name}\"\n",
    "                    logging.info(f\"Processing field {processed_fields + 1}/{total_fields}: {full_field_name}\")\n",
    "                    if full_field_name in all_test_cases:\n",
    "                        logging.warning(f\"Skipping {full_field_name}, already processed.\")\n",
    "                        continue\n",
    "            # # Process each field\n",
    "            # for parent_field, details in rules.items():\n",
    "            #     for field_name, field_details in details[\"fields\"].items():\n",
    "            #         full_field_name = f\"{parent_field}.{field_name}\"\n",
    "            #         logging.info(f\"Processing field {processed_fields + 1}/{total_fields}: {full_field_name}\")\n",
    "\n",
    "                    # Generate prompt\n",
    "                    prompt = self._generate_prompt(\n",
    "                        field_name,\n",
    "                        field_details[\"data_type\"],\n",
    "                        field_details[\"constraints\"],\n",
    "                        field_details.get(\"description\", \"\")\n",
    "                    )\n",
    "\n",
    "                    # Get LLM response with retries\n",
    "                    max_retries = 3\n",
    "                    for attempt in range(max_retries):\n",
    "                        try:\n",
    "                            response = self.llm_client.generate_content(prompt)\n",
    "                            test_cases = self._parse_llm_response(response.text, field_details[\"data_type\"])\n",
    "                            \n",
    "                            if test_cases:\n",
    "                                all_test_cases[full_field_name] = test_cases\n",
    "                                logging.info(f\"Successfully generated {len(test_cases)} test cases\")\n",
    "                                break\n",
    "                            else:\n",
    "                                logging.warning(f\"Attempt {attempt + 1}: Failed to generate valid test cases\")\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                            if attempt == max_retries - 1:\n",
    "                                logging.error(f\"Failed to generate test cases for {full_field_name} after {max_retries} attempts\")\n",
    "                    \n",
    "                    processed_fields += 1\n",
    "\n",
    "            # Save results\n",
    "            self._save_test_cases(all_test_cases, output_file)\n",
    "            \n",
    "            # Generate summary\n",
    "            self._generate_summary(all_test_cases, output_file)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate test cases: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _save_test_cases(self, test_cases: Dict[str, List[Dict[str, Any]]], output_file: str) -> None:\n",
    "        \"\"\"Save test cases with backup.\"\"\"\n",
    "        try:\n",
    "            # Create backup of existing file if it exists\n",
    "            if os.path.exists(output_file):\n",
    "                backup_file = f\"{output_file}.{datetime.now().strftime('%Y%m%d_%H%M%S')}.bak\"\n",
    "                os.rename(output_file, backup_file)\n",
    "                logging.info(f\"Created backup: {backup_file}\")\n",
    "\n",
    "            # Save new test cases\n",
    "            with open(output_file, \"w\") as f:\n",
    "                json.dump(test_cases, f, indent=2)\n",
    "            logging.info(f\"Successfully saved test cases to {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save test cases: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _generate_summary(self, test_cases: Dict[str, List[Dict[str, Any]]], output_file: str) -> None:\n",
    "        \"\"\"Generate a summary of the test case generation.\"\"\"\n",
    "        total_fields = len(test_cases)\n",
    "        total_test_cases = sum(len(cases) for cases in test_cases.values())\n",
    "        \n",
    "        summary = (\n",
    "            f\"\\nTest Case Generation Summary\\n\"\n",
    "            f\"{'='*30}\\n\"\n",
    "            f\"Total fields processed: {total_fields}\\n\"\n",
    "            f\"Total test cases generated: {total_test_cases}\\n\"\n",
    "            f\"Average test cases per field: {total_test_cases/total_fields:.2f}\\n\"\n",
    "            f\"Output file: {output_file}\\n\"\n",
    "            f\"{'='*30}\"\n",
    "        )\n",
    "        \n",
    "        logging.info(summary)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        generator = TestCaseGenerator()\n",
    "        generator.generate_test_cases(\n",
    "            generator.config[\"constrains_processed_rules_file\"],\n",
    "            generator.config[\"generated_test_cases_file\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Application failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
