{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nitastha/Desktop/NitishFiles/Work/Optum/project\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 11:46:55,950 - INFO - Processing field 1/10: Rx Bc Demographics.Rx BC Email\n",
      "2025-02-17 11:47:03,573 - ERROR - JSON parsing error: Invalid \\escape: line 54 column 37 (char 1503)\n",
      "2025-02-17 11:47:03,574 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 11:47:09,708 - INFO - Successfully generated 14 test cases\n",
      "2025-02-17 11:47:09,709 - INFO - Processing field 2/10: Rx Bc Demographics.Rx BC First Name\n",
      "2025-02-17 11:47:14,191 - ERROR - JSON parsing error: Expecting ',' delimiter: line 66 column 20 (char 1770)\n",
      "2025-02-17 11:47:14,192 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 11:47:18,208 - INFO - Successfully generated 10 test cases\n",
      "2025-02-17 11:47:18,208 - INFO - Processing field 3/10: Rx Bc Demographics.Rx BC Last Name\n",
      "2025-02-17 11:47:23,415 - INFO - Successfully generated 13 test cases\n",
      "2025-02-17 11:47:23,416 - INFO - Processing field 4/10: Rx BC Email Event.Rx BC Email\n",
      "2025-02-17 11:47:29,061 - ERROR - JSON parsing error: Invalid \\escape: line 54 column 37 (char 1530)\n",
      "2025-02-17 11:47:29,062 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 11:47:34,386 - ERROR - JSON parsing error: Invalid \\escape: line 48 column 37 (char 1339)\n",
      "2025-02-17 11:47:34,388 - WARNING - Attempt 2: Failed to generate valid test cases\n",
      "2025-02-17 11:47:39,954 - ERROR - JSON parsing error: Invalid \\escape: line 48 column 37 (char 1345)\n",
      "2025-02-17 11:47:39,955 - WARNING - Attempt 3: Failed to generate valid test cases\n",
      "2025-02-17 11:47:39,955 - INFO - Processing field 5/10: Rx BC Email Event.Rx BC Email Template Info\n",
      "2025-02-17 11:47:44,681 - ERROR - JSON parsing error: Invalid \\escape: line 36 column 37 (char 1117)\n",
      "2025-02-17 11:47:44,682 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 11:47:49,720 - ERROR - JSON parsing error: Invalid \\escape: line 18 column 89 (char 744)\n",
      "2025-02-17 11:47:49,721 - WARNING - Attempt 2: Failed to generate valid test cases\n",
      "2025-02-17 11:47:55,271 - INFO - Successfully generated 11 test cases\n",
      "2025-02-17 11:47:55,272 - INFO - Processing field 6/10: Rx BC Email Event.Rx BC Event ID\n",
      "2025-02-17 11:48:00,294 - INFO - Successfully generated 12 test cases\n",
      "2025-02-17 11:48:00,295 - INFO - Processing field 7/10: Rx BC Email Event.Rx BC Event Type\n",
      "2025-02-17 11:48:05,306 - INFO - Successfully generated 12 test cases\n",
      "2025-02-17 11:48:05,307 - INFO - Processing field 8/10: Rx BC Email Event.Rx BC New User Registration Link\n",
      "2025-02-17 11:48:10,840 - ERROR - JSON parsing error: Invalid \\escape: line 42 column 61 (char 1307)\n",
      "2025-02-17 11:48:10,841 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 11:48:16,371 - INFO - Successfully generated 10 test cases\n",
      "2025-02-17 11:48:16,372 - INFO - Processing field 9/10: Rx BC Email Event.Rx BC Timestamp\n",
      "2025-02-17 11:48:23,436 - INFO - Successfully generated 14 test cases\n",
      "2025-02-17 11:48:23,437 - INFO - Processing field 10/10: Rx BC Email Event.Rx BC User Group\n",
      "2025-02-17 11:48:27,942 - INFO - Successfully generated 11 test cases\n",
      "2025-02-17 11:48:27,945 - INFO - Successfully saved test cases to data/anth.json\n",
      "2025-02-17 11:48:27,946 - INFO - \n",
      "Test Case Generation Summary\n",
      "==============================\n",
      "Total fields processed: 9\n",
      "Total test cases generated: 107\n",
      "Average test cases per field: 11.89\n",
      "Output file: data/anth.json\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional, Any\n",
    "import google.generativeai as genai\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('test_generation.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self, config_path: str = \"config/settings.yaml\"):\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.llm_client = self._initialize_llm()\n",
    "        \n",
    "    def _load_config(self, config_path: str) -> dict:\n",
    "        \"\"\"Load configuration from YAML file with error handling.\"\"\"\n",
    "        try:\n",
    "            with open(config_path, \"r\") as f:\n",
    "                return yaml.safe_load(f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load config: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_llm(self) -> genai.GenerativeModel:\n",
    "        \"\"\"Initialize the LLM client with error handling.\"\"\"\n",
    "        try:\n",
    "            if not self.config.get(\"gemini_api_key\"):\n",
    "                raise ValueError(\"Gemini API key not found in config\")\n",
    "            \n",
    "            genai.configure(api_key=self.config[\"gemini_api_key\"])\n",
    "            model_name = self.config.get(\"gemini_model\", \"gemini-1.5-flash\")\n",
    "            return genai.GenerativeModel(model_name)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize LLM: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _generate_prompt(self, field_name: str, data_type: str, constraints: List[str]) -> str:\n",
    "        \"\"\"Generate a more structured and specific prompt for test case generation.\"\"\"\n",
    "        return f\"\"\"\n",
    "Generate test cases for the field '{field_name}' with following specifications:\n",
    "- Data Type: {data_type}\n",
    "- Constraints: {constraints}\n",
    "\n",
    "Requirements:\n",
    "1. Include ONLY the JSON array of test cases in your response\n",
    "2. Each test case must have these exact fields:\n",
    "   - \"test_case\": A clear, unique identifier for the test\n",
    "   - \"description\": Detailed explanation of what the test verifies\n",
    "   - \"expected_result\": MUST be exactly \"Pass\" or \"Fail\"\n",
    "   - \"input\": The test input value (can be null, string, number, etc.)\n",
    "\n",
    "3. Include these types of test cases:\n",
    "   - Basic valid inputs\n",
    "   - Basic invalid inputs\n",
    "   - Null/empty handling\n",
    "   - Boundary conditions\n",
    "   - Edge cases\n",
    "   - Type validation\n",
    "\n",
    "Return the response in this exact format:\n",
    "[\n",
    "    {{\n",
    "        \"test_case\": \"TC001_Valid_Basic\",\n",
    "        \"description\": \"Basic valid input test\",\n",
    "        \"expected_result\": \"Pass\",\n",
    "        \"input\": \"example\"\n",
    "    }},\n",
    "    {{\n",
    "        \"test_case\": \"TC002_Invalid_Null\",\n",
    "        \"description\": \"Test with null input\",\n",
    "        \"expected_result\": \"Fail\",\n",
    "        \"input\": null\n",
    "    }}\n",
    "]\n",
    "\n",
    "IMPORTANT: Return ONLY the JSON array. No additional text or explanation.\"\"\"\n",
    "\n",
    "    def _parse_llm_response(self, response_text: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        \"\"\"Parse and validate LLM response with improved error handling.\"\"\"\n",
    "        try:\n",
    "            # Remove any markdown code blocks if present\n",
    "            cleaned_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            test_cases = json.loads(cleaned_text)\n",
    "            \n",
    "            # Validate structure\n",
    "            if not isinstance(test_cases, list):\n",
    "                raise ValueError(\"Response is not a JSON array\")\n",
    "            \n",
    "            # Validate and normalize each test case\n",
    "            validated_cases = []\n",
    "            for idx, case in enumerate(test_cases, 1):\n",
    "                required_fields = {\"test_case\", \"description\", \"expected_result\", \"input\"}\n",
    "                if not all(field in case for field in required_fields):\n",
    "                    logging.warning(f\"Test case {idx} missing required fields, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Normalize expected_result to Pass/Fail\n",
    "                case[\"expected_result\"] = \"Pass\" if case[\"expected_result\"].lower() == \"pass\" else \"Fail\"\n",
    "                validated_cases.append(case)\n",
    "            \n",
    "            return validated_cases\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"JSON parsing error: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error parsing response: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def generate_test_cases(self, rules_file: str, output_file: str) -> None:\n",
    "        \"\"\"Main method to generate and save test cases.\"\"\"\n",
    "        try:\n",
    "            # Load rules\n",
    "            with open(rules_file, \"r\") as f:\n",
    "                rules = json.load(f)\n",
    "\n",
    "            all_test_cases = {}\n",
    "            total_fields = sum(len(details[\"fields\"]) for details in rules.values())\n",
    "            processed_fields = 0\n",
    "\n",
    "            # Process each field\n",
    "            for parent_field, details in rules.items():\n",
    "                for field_name, field_details in details[\"fields\"].items():\n",
    "                    full_field_name = f\"{parent_field}.{field_name}\"\n",
    "                    logging.info(f\"Processing field {processed_fields + 1}/{total_fields}: {full_field_name}\")\n",
    "\n",
    "                    # Generate prompt\n",
    "                    prompt = self._generate_prompt(\n",
    "                        field_name,\n",
    "                        field_details[\"data_type\"],\n",
    "                        field_details[\"constraints\"]\n",
    "                    )\n",
    "\n",
    "                    # Get LLM response with retries\n",
    "                    max_retries = 3\n",
    "                    for attempt in range(max_retries):\n",
    "                        try:\n",
    "                            response = self.llm_client.generate_content(prompt)\n",
    "                            test_cases = self._parse_llm_response(response.text)\n",
    "                            \n",
    "                            if test_cases:\n",
    "                                all_test_cases[full_field_name] = test_cases\n",
    "                                logging.info(f\"Successfully generated {len(test_cases)} test cases\")\n",
    "                                break\n",
    "                            else:\n",
    "                                logging.warning(f\"Attempt {attempt + 1}: Failed to generate valid test cases\")\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                            if attempt == max_retries - 1:\n",
    "                                logging.error(f\"Failed to generate test cases for {full_field_name} after {max_retries} attempts\")\n",
    "                    \n",
    "                    processed_fields += 1\n",
    "\n",
    "            # Save results\n",
    "            self._save_test_cases(all_test_cases, output_file)\n",
    "            \n",
    "            # Generate summary\n",
    "            self._generate_summary(all_test_cases, output_file)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate test cases: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _save_test_cases(self, test_cases: Dict[str, List[Dict[str, Any]]], output_file: str) -> None:\n",
    "        \"\"\"Save test cases with backup.\"\"\"\n",
    "        try:\n",
    "            # Create backup of existing file if it exists\n",
    "            if os.path.exists(output_file):\n",
    "                backup_file = f\"{output_file}.{datetime.now().strftime('%Y%m%d_%H%M%S')}.bak\"\n",
    "                os.rename(output_file, backup_file)\n",
    "                logging.info(f\"Created backup: {backup_file}\")\n",
    "\n",
    "            # Save new test cases\n",
    "            with open(output_file, \"w\") as f:\n",
    "                json.dump(test_cases, f, indent=2)\n",
    "            logging.info(f\"Successfully saved test cases to {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save test cases: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _generate_summary(self, test_cases: Dict[str, List[Dict[str, Any]]], output_file: str) -> None:\n",
    "        \"\"\"Generate a summary of the test case generation.\"\"\"\n",
    "        total_fields = len(test_cases)\n",
    "        total_test_cases = sum(len(cases) for cases in test_cases.values())\n",
    "        \n",
    "        summary = (\n",
    "            f\"\\nTest Case Generation Summary\\n\"\n",
    "            f\"{'='*30}\\n\"\n",
    "            f\"Total fields processed: {total_fields}\\n\"\n",
    "            f\"Total test cases generated: {total_test_cases}\\n\"\n",
    "            f\"Average test cases per field: {total_test_cases/total_fields:.2f}\\n\"\n",
    "            f\"Output file: {output_file}\\n\"\n",
    "            f\"{'='*30}\"\n",
    "        )\n",
    "        \n",
    "        logging.info(summary)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        generator = TestCaseGenerator()\n",
    "        generator.generate_test_cases(\n",
    "            generator.config[\"constrains_processed_rules_file\"],\n",
    "            generator.config[\"generated_test_cases_file\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Application failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 15:29:42,126 - INFO - Processing field 1/10: Rx Bc Demographics.Rx BC Email\n",
      "2025-02-17 15:29:50,073 - INFO - Successfully generated 14 test cases\n",
      "2025-02-17 15:29:50,074 - INFO - Processing field 2/10: Rx Bc Demographics.Rx BC First Name\n",
      "2025-02-17 15:29:55,202 - ERROR - JSON parsing error: Expecting ',' delimiter: line 54 column 17 (char 1443) - Raw response: ```json\n",
      "[\n",
      "  {\n",
      "    \"test_case\": \"TC001_Valid_Basic\",\n",
      "    \"description\": \"Basic valid input test\",\n",
      "    \"expected_result\": \"Pass\",\n",
      "    \"input\": \"John\"\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC002_Valid_LongName\",\n",
      "    \"description\": \"Valid input with a long name\",\n",
      "    \"expected_result\": \"Pass\",\n",
      "    \"input\": \"JohnathanChristopherSmith\"\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC003_Valid_SpecialChars\",\n",
      "    \"description\": \"Valid input with special characters allowed (if any)\",\n",
      "    \"expected_result\": \"Pass\",\n",
      "    \"input\": \"John O'Malley\"\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC004_Invalid_Null\",\n",
      "    \"description\": \"Null input test\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": null\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC005_Invalid_Empty\",\n",
      "    \"description\": \"Empty string input test\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC006_Invalid_Whitespace\",\n",
      "    \"description\": \"Input with only whitespace\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": \"   \"\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC007_Invalid_NumbersOnly\",\n",
      "    \"description\": \"Input with only numbers\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": \"12345\"\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC008_Invalid_SpecialChars\",\n",
      "    \"description\": \"Input with disallowed special characters\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": \"John !@#$%^&*()\"\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC009_Boundary_Maxlength\",\n",
      "    \"description\": \"Input at the maximum length (assuming a limit exists)\",\n",
      "    \"expected_result\": \"Pass\",\n",
      "    \"input\": \"A\".repeat(255) // Adjust 255 to the actual maximum length if known\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC010_Boundary_ExceedingMaxlength\",\n",
      "    \"description\": \"Input exceeding the maximum length (assuming a limit exists)\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": \"A\".repeat(256) // Adjust 256 to the actual maximum length +1 if known\n",
      "\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC011_Edge_OnlyLetters\",\n",
      "    \"description\": \"Input with only letters\",\n",
      "    \"expected_result\": \"Pass\",\n",
      "    \"input\": \"ABCDEFG\"\n",
      "  },\n",
      "    {\n",
      "    \"test_case\": \"TC012_Type_Number\",\n",
      "    \"description\": \"Input of type number\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": 123\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "2025-02-17 15:29:55,203 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 15:29:59,966 - INFO - Successfully generated 12 test cases\n",
      "2025-02-17 15:29:59,967 - INFO - Processing field 3/10: Rx Bc Demographics.Rx BC Last Name\n",
      "2025-02-17 15:30:04,582 - INFO - Successfully generated 10 test cases\n",
      "2025-02-17 15:30:04,582 - INFO - Processing field 4/10: Rx BC Email Event.Rx BC Email\n",
      "2025-02-17 15:30:09,530 - INFO - Successfully generated 11 test cases\n",
      "2025-02-17 15:30:09,531 - INFO - Processing field 5/10: Rx BC Email Event.Rx BC Email Template Info\n",
      "2025-02-17 15:30:14,709 - ERROR - JSON parsing error: Expecting ',' delimiter: line 66 column 17 (char 1861) - Raw response: ```json\n",
      "[\n",
      "  {\n",
      "    \"test_case\": \"TC001_Valid_Basic\",\n",
      "    \"description\": \"Basic valid input test\",\n",
      "    \"expected_result\": \"Pass\",\n",
      "    \"input\": \"Valid Email Template Info\"\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC002_Valid_LongString\",\n",
      "    \"description\": \"Valid input with a long string (check for length limits)\",\n",
      "    \"expected_result\": \"Pass\",\n",
      "    \"input\": \"This is a long string to test the length limit of the Rx BC Email Template Info field.  It should be long enough to cover most scenarios.\"\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC003_Valid_SpecialCharacters\",\n",
      "    \"description\": \"Valid input with special characters\",\n",
      "    \"expected_result\": \"Pass\",\n",
      "    \"input\": \"Valid!@#$%^&*()-+={}[]|;:'\\\",./<>? Template Info\"\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC004_Invalid_Null\",\n",
      "    \"description\": \"Null input test\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": null\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC005_Invalid_Empty\",\n",
      "    \"description\": \"Empty string input test\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC006_Invalid_Whitespace\",\n",
      "    \"description\": \"Whitespace only input test\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": \"   \"\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC007_Invalid_Number\",\n",
      "    \"description\": \"Numeric input test\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": 123\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC008_Invalid_Boolean\",\n",
      "    \"description\": \"Boolean input test\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": true\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC009_Invalid_Array\",\n",
      "    \"description\": \"Array input test\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": []\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC010_Invalid_Object\",\n",
      "    \"description\": \"Object input test\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": {}\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC011_Boundary_Max_Length\",\n",
      "    \"description\": \"Input at the maximum length limit (assuming a length limit exists)\",\n",
      "    \"expected_result\": \"Pass\",\n",
      "    \"input\": \"a\".repeat(1000) // Replace 1000 with actual length limit if known\n",
      "  },\n",
      "  {\n",
      "    \"test_case\": \"TC012_Boundary_Exceeds_MaxLength\",\n",
      "    \"description\": \"Input exceeding the maximum length limit (assuming a length limit exists)\",\n",
      "    \"expected_result\": \"Fail\",\n",
      "    \"input\": \"a\".repeat(1001) // Replace 1001 with actual length limit + 1 if known\n",
      "\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "2025-02-17 15:30:14,710 - WARNING - Attempt 1: Failed to generate valid test cases\n",
      "2025-02-17 15:30:18,915 - INFO - Successfully generated 10 test cases\n",
      "2025-02-17 15:30:18,916 - INFO - Processing field 6/10: Rx BC Email Event.Rx BC Event ID\n",
      "2025-02-17 15:30:23,107 - INFO - Successfully generated 10 test cases\n",
      "2025-02-17 15:30:23,108 - INFO - Processing field 7/10: Rx BC Email Event.Rx BC Event Type\n",
      "2025-02-17 15:30:27,510 - INFO - Successfully generated 11 test cases\n",
      "2025-02-17 15:30:27,512 - INFO - Processing field 8/10: Rx BC Email Event.Rx BC New User Registration Link\n",
      "2025-02-17 15:30:32,732 - INFO - Successfully generated 10 test cases\n",
      "2025-02-17 15:30:32,734 - INFO - Processing field 9/10: Rx BC Email Event.Rx BC Timestamp\n",
      "2025-02-17 15:30:40,625 - WARNING - Test case 4 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:30:40,626 - WARNING - Test case 5 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:30:40,627 - WARNING - Test case 7 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:30:40,627 - WARNING - Test case 11 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:30:40,628 - WARNING - Test case 12 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:30:40,629 - WARNING - Test case 13 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:30:40,631 - WARNING - Test case 14 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:30:40,632 - WARNING - Test case 15 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:30:40,633 - WARNING - Test case 16 validation failed: Invalid date format. Expected formats: ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
      "2025-02-17 15:30:40,634 - INFO - Successfully generated 7 test cases\n",
      "2025-02-17 15:30:40,635 - INFO - Processing field 10/10: Rx BC Email Event.Rx BC User Group\n",
      "2025-02-17 15:30:44,819 - INFO - Successfully generated 10 test cases\n",
      "2025-02-17 15:30:44,822 - INFO - Created backup: data/anth1.json.20250217_153044.bak\n",
      "2025-02-17 15:30:44,824 - INFO - Successfully saved test cases to data/anth1.json\n",
      "2025-02-17 15:30:44,825 - INFO - \n",
      "Test Case Generation Summary\n",
      "==============================\n",
      "Total fields processed: 10\n",
      "Total test cases generated: 105\n",
      "Average test cases per field: 10.50\n",
      "Output file: data/anth1.json\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "import google.generativeai as genai\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('test_generation.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self, config_path: str = \"config/settings.yaml\"):\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.llm_client = self._initialize_llm()\n",
    "        self.field_specific_rules = self._initialize_field_rules()\n",
    "        \n",
    "    def _load_config(self, config_path: str) -> dict:\n",
    "        \"\"\"Load configuration from YAML file with error handling.\"\"\"\n",
    "        try:\n",
    "            with open(config_path, \"r\") as f:\n",
    "                return yaml.safe_load(f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load config: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_llm(self) -> genai.GenerativeModel:\n",
    "        \"\"\"Initialize the LLM client with error handling.\"\"\"\n",
    "        try:\n",
    "            if not self.config.get(\"gemini_api_key\"):\n",
    "                raise ValueError(\"Gemini API key not found in config\")\n",
    "            \n",
    "            genai.configure(api_key=self.config[\"gemini_api_key\"])\n",
    "            model_name = self.config.get(\"gemini_model\", \"gemini-1.5-flash\")\n",
    "            return genai.GenerativeModel(model_name)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize LLM: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_field_rules(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Initialize specific rules for different field types.\"\"\"\n",
    "        return {\n",
    "            \"Date\": {\n",
    "                \"valid_formats\": [\n",
    "                    \"%Y-%m-%d %H:%M:%S\",\n",
    "                    \"%Y/%m/%d %H:%M:%S\",\n",
    "                    \"%m/%d/%Y %H:%M:%S\"\n",
    "                ],\n",
    "                \"extra_validation\": self._validate_date_format\n",
    "            },\n",
    "            \"String\": {\n",
    "                \"extra_validation\": self._validate_string_format\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _validate_date_format(self, test_case: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate date format test cases.\"\"\"\n",
    "        if test_case[\"input\"] is None:\n",
    "            return True, \"\"\n",
    "            \n",
    "        if isinstance(test_case[\"input\"], str):\n",
    "            for date_format in self.field_specific_rules[\"Date\"][\"valid_formats\"]:\n",
    "                try:\n",
    "                    datetime.strptime(test_case[\"input\"], date_format)\n",
    "                    return True, \"\"\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            return False, f\"Invalid date format. Expected formats: {self.field_specific_rules['Date']['valid_formats']}\"\n",
    "        return False, \"Date input must be a string\"\n",
    "\n",
    "    def _validate_string_format(self, test_case: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate string format test cases.\"\"\"\n",
    "        if test_case[\"input\"] is None:\n",
    "            return True, \"\"\n",
    "            \n",
    "        if not isinstance(test_case[\"input\"], (str, type(None))):\n",
    "            if test_case[\"expected_result\"] == \"Pass\":\n",
    "                return False, \"String field with non-string input should fail\"\n",
    "        return True, \"\"\n",
    "\n",
    "    def _generate_prompt(self, field_name: str, data_type: str, constraints: List[str], description: str = \"\") -> str:\n",
    "        \"\"\"Generate a more structured and specific prompt for test case generation.\"\"\"\n",
    "        field_specific_info = \"\"\n",
    "        if data_type == \"Date\":\n",
    "            field_specific_info = \"\\nFor Date fields, use these formats only:\\n\" + \\\n",
    "                                \"\\n\".join(f\"- {fmt}\" for fmt in self.field_specific_rules[\"Date\"][\"valid_formats\"])\n",
    "\n",
    "        return f\"\"\"\n",
    "Generate test cases for the field '{field_name}' with following specifications:\n",
    "- Data Type: {data_type}\n",
    "- Constraints: {constraints}\n",
    "- Description: {description}{field_specific_info}\n",
    "\n",
    "Requirements:\n",
    "1. Include ONLY the JSON array of test cases in your response\n",
    "2. Each test case must have these exact fields:\n",
    "   - \"test_case\": A clear, unique identifier for the test\n",
    "   - \"description\": Detailed explanation of what the test verifies\n",
    "   - \"expected_result\": MUST be exactly \"Pass\" or \"Fail\"\n",
    "   - \"input\": The test input value (can be null, string, number, etc.)\n",
    "\n",
    "3. Include these types of test cases:\n",
    "   - Basic valid inputs\n",
    "   - Basic invalid inputs\n",
    "   - Null/empty handling\n",
    "   - Boundary conditions\n",
    "   - Edge cases\n",
    "   - Type validation\n",
    "\n",
    "4. Consider field-specific requirements:\n",
    "   - For Date fields: Include only valid date formats specified\n",
    "   - For String fields: Consider length limits and character restrictions\n",
    "   - Handle nullable fields appropriately based on constraints\n",
    "\n",
    "Return the response in this exact format:\n",
    "[\n",
    "    {{\n",
    "        \"test_case\": \"TC001_Valid_Basic\",\n",
    "        \"description\": \"Basic valid input test\",\n",
    "        \"expected_result\": \"Pass\",\n",
    "        \"input\": \"example\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "IMPORTANT: Return ONLY the JSON array. No additional text or explanation.\"\"\"\n",
    "\n",
    "    def _validate_test_case(self, test_case: Dict[str, Any], data_type: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate a single test case based on field type and rules.\"\"\"\n",
    "        if not all(field in test_case for field in [\"test_case\", \"description\", \"expected_result\", \"input\"]):\n",
    "            return False, \"Missing required fields\"\n",
    "\n",
    "        if test_case[\"expected_result\"] not in [\"Pass\", \"Fail\"]:\n",
    "            return False, \"Invalid expected_result value\"\n",
    "\n",
    "        # Apply field-specific validation\n",
    "        if data_type in self.field_specific_rules:\n",
    "            return self.field_specific_rules[data_type][\"extra_validation\"](test_case)\n",
    "\n",
    "        return True, \"\"\n",
    "\n",
    "    def _parse_llm_response(self, response_text: str, data_type: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        \"\"\"Parse and validate LLM response with improved error handling.\"\"\"\n",
    "        try:\n",
    "            # Remove Markdown JSON blocks if present\n",
    "            cleaned_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "            # Handle invalid escape sequences\n",
    "            cleaned_text = re.sub(r'\\\\([^\"\\\\])', r'\\\\\\\\\\1', cleaned_text)  \n",
    "\n",
    "            # Parse JSON\n",
    "            test_cases = json.loads(cleaned_text)\n",
    "\n",
    "            # Validate structure\n",
    "            if not isinstance(test_cases, list):\n",
    "                raise ValueError(\"Response is not a JSON array\")\n",
    "\n",
    "            # Validate and normalize each test case\n",
    "            validated_cases = []\n",
    "            for idx, case in enumerate(test_cases, 1):\n",
    "                is_valid, error_msg = self._validate_test_case(case, data_type)\n",
    "                if not is_valid:\n",
    "                    logging.warning(f\"Test case {idx} validation failed: {error_msg}\")\n",
    "                    continue\n",
    "                \n",
    "                # Normalize expected_result to Pass/Fail\n",
    "                case[\"expected_result\"] = \"Pass\" if case[\"expected_result\"].lower() == \"pass\" else \"Fail\"\n",
    "                validated_cases.append(case)\n",
    "\n",
    "            return validated_cases\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"JSON parsing error: {str(e)} - Raw response: {response_text}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error parsing response: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def generate_test_cases(self, rules_file: str, output_file: str) -> None:\n",
    "        \"\"\"Main method to generate and save test cases.\"\"\"\n",
    "        try:\n",
    "            # Load rules\n",
    "            with open(rules_file, \"r\") as f:\n",
    "                rules = json.load(f)\n",
    "\n",
    "            all_test_cases = {}\n",
    "            total_fields = sum(len(details[\"fields\"]) for details in rules.values())\n",
    "            processed_fields = 0\n",
    "\n",
    "\n",
    "            for parent_field, details in rules.items():\n",
    "                for field_name, field_details in details[\"fields\"].items():\n",
    "                    full_field_name = f\"{parent_field}.{field_name}\"\n",
    "                    logging.info(f\"Processing field {processed_fields + 1}/{total_fields}: {full_field_name}\")\n",
    "                    if full_field_name in all_test_cases:\n",
    "                        logging.warning(f\"Skipping {full_field_name}, already processed.\")\n",
    "                        continue\n",
    "            # # Process each field\n",
    "            # for parent_field, details in rules.items():\n",
    "            #     for field_name, field_details in details[\"fields\"].items():\n",
    "            #         full_field_name = f\"{parent_field}.{field_name}\"\n",
    "            #         logging.info(f\"Processing field {processed_fields + 1}/{total_fields}: {full_field_name}\")\n",
    "\n",
    "                    # Generate prompt\n",
    "                    prompt = self._generate_prompt(\n",
    "                        field_name,\n",
    "                        field_details[\"data_type\"],\n",
    "                        field_details[\"constraints\"],\n",
    "                        field_details.get(\"description\", \"\")\n",
    "                    )\n",
    "\n",
    "                    # Get LLM response with retries\n",
    "                    max_retries = 3\n",
    "                    for attempt in range(max_retries):\n",
    "                        try:\n",
    "                            response = self.llm_client.generate_content(prompt)\n",
    "                            test_cases = self._parse_llm_response(response.text, field_details[\"data_type\"])\n",
    "                            \n",
    "                            if test_cases:\n",
    "                                all_test_cases[full_field_name] = test_cases\n",
    "                                logging.info(f\"Successfully generated {len(test_cases)} test cases\")\n",
    "                                break\n",
    "                            else:\n",
    "                                logging.warning(f\"Attempt {attempt + 1}: Failed to generate valid test cases\")\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                            if attempt == max_retries - 1:\n",
    "                                logging.error(f\"Failed to generate test cases for {full_field_name} after {max_retries} attempts\")\n",
    "                    \n",
    "                    processed_fields += 1\n",
    "\n",
    "            # Save results\n",
    "            self._save_test_cases(all_test_cases, output_file)\n",
    "            \n",
    "            # Generate summary\n",
    "            self._generate_summary(all_test_cases, output_file)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate test cases: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _save_test_cases(self, test_cases: Dict[str, List[Dict[str, Any]]], output_file: str) -> None:\n",
    "        \"\"\"Save test cases with backup.\"\"\"\n",
    "        try:\n",
    "            # Create backup of existing file if it exists\n",
    "            if os.path.exists(output_file):\n",
    "                backup_file = f\"{output_file}.{datetime.now().strftime('%Y%m%d_%H%M%S')}.bak\"\n",
    "                os.rename(output_file, backup_file)\n",
    "                logging.info(f\"Created backup: {backup_file}\")\n",
    "\n",
    "            # Save new test cases\n",
    "            with open(output_file, \"w\") as f:\n",
    "                json.dump(test_cases, f, indent=2)\n",
    "            logging.info(f\"Successfully saved test cases to {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save test cases: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _generate_summary(self, test_cases: Dict[str, List[Dict[str, Any]]], output_file: str) -> None:\n",
    "        \"\"\"Generate a summary of the test case generation.\"\"\"\n",
    "        total_fields = len(test_cases)\n",
    "        total_test_cases = sum(len(cases) for cases in test_cases.values())\n",
    "        \n",
    "        summary = (\n",
    "            f\"\\nTest Case Generation Summary\\n\"\n",
    "            f\"{'='*30}\\n\"\n",
    "            f\"Total fields processed: {total_fields}\\n\"\n",
    "            f\"Total test cases generated: {total_test_cases}\\n\"\n",
    "            f\"Average test cases per field: {total_test_cases/total_fields:.2f}\\n\"\n",
    "            f\"Output file: {output_file}\\n\"\n",
    "            f\"{'='*30}\"\n",
    "        )\n",
    "        \n",
    "        logging.info(summary)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        generator = TestCaseGenerator()\n",
    "        generator.generate_test_cases(\n",
    "            generator.config[\"constrains_processed_rules_file\"],\n",
    "            generator.config[\"generated_test_cases_file\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Application failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 16:36:13,685 - INFO - Successfully saved updated test cases to key_test.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import uuid\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('add_keys.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def add_unique_keys(input_file: str, output_file: str):\n",
    "    \"\"\"Read test cases, add unique keys, and save to a new file.\"\"\"\n",
    "    try:\n",
    "        # Load JSON file\n",
    "        with open(input_file, \"r\") as f:\n",
    "            test_cases = json.load(f)\n",
    "        \n",
    "        # Process each field and test case\n",
    "        for field_name, cases in test_cases.items():\n",
    "            for case in cases:\n",
    "                case[\"key\"] = str(uuid.uuid4())  # Assign a unique UUID\n",
    "        \n",
    "        # Save updated test cases with a backup mechanism\n",
    "        if os.path.exists(output_file):\n",
    "            backup_file = f\"{output_file}.{uuid.uuid4().hex}.bak\"\n",
    "            os.rename(output_file, backup_file)\n",
    "            logging.info(f\"Backup created: {backup_file}\")\n",
    "        \n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(test_cases, f, indent=2)\n",
    "        \n",
    "        logging.info(f\"Successfully saved updated test cases to {output_file}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing test cases: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    input_file = \"data/anth1.json\"\n",
    "    output_file = \"key_test.json\"\n",
    "    add_unique_keys(input_file, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the DataFrame:\n",
      "\n",
      "Test case statistics:\n",
      "Total number of test cases: 105\n",
      "Number of unique categories: 10\n",
      "Number of pass/fail cases: \n",
      "expected_result\n",
      "Fail    54\n",
      "Pass    51\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "def flatten_test_cases(data: Dict) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Flattens nested JSON test cases into a list of dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): The nested JSON data\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of flattened test case dictionaries\n",
    "    \"\"\"\n",
    "    flattened_data = []\n",
    "    \n",
    "    # Iterate through each main category\n",
    "    for category, test_cases in data.items():\n",
    "        # Extract the main category name (e.g., \"Rx Bc Demographics.Rx BC Email\")\n",
    "        \n",
    "        # Iterate through each test case in the category\n",
    "        for test_case in test_cases:\n",
    "            # Create a new dictionary with all fields plus the category\n",
    "            test_case_dict = {\n",
    "                'category': category,\n",
    "                'test_case': test_case['test_case'],\n",
    "                'description': test_case['description'],\n",
    "                'expected_result': test_case['expected_result'],\n",
    "                'input': str(test_case['input']),  # Convert input to string to handle various types\n",
    "                'key': test_case['key']\n",
    "            }\n",
    "            flattened_data.append(test_case_dict)\n",
    "    \n",
    "    return flattened_data\n",
    "\n",
    "# Read and process the JSON\n",
    "def process_test_cases(json_data: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process JSON test cases and convert to a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        json_data (str): The JSON data as a string\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all test cases\n",
    "    \"\"\"\n",
    "    # Parse JSON\n",
    "    data = json.loads(json_data)\n",
    "    \n",
    "    # Flatten the data\n",
    "    flattened_data = flatten_test_cases(data)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    column_order = ['category', 'test_case', 'description', 'expected_result', 'input', 'key']\n",
    "    df = df[column_order]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "with open('key_test.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "\n",
    "# Create DataFrame\n",
    "df = process_test_cases(json_data)\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst few rows of the DataFrame:\")\n",
    "\n",
    "\n",
    "# Get some basic statistics\n",
    "print(\"\\nTest case statistics:\")\n",
    "print(f\"Total number of test cases: {len(df)}\")\n",
    "print(f\"Number of unique categories: {df['category'].nunique()}\")\n",
    "print(f\"Number of pass/fail cases: \\n{df['expected_result'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>test_case</th>\n",
       "      <th>description</th>\n",
       "      <th>expected_result</th>\n",
       "      <th>input</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rx Bc Demographics.Rx BC Email</td>\n",
       "      <td>TC001_Valid_Basic</td>\n",
       "      <td>Basic valid input test</td>\n",
       "      <td>Pass</td>\n",
       "      <td>test@example.com</td>\n",
       "      <td>3686623b-2261-4c79-8c10-6b6f8bc0824c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rx Bc Demographics.Rx BC Email</td>\n",
       "      <td>TC002_Valid_LongEmail</td>\n",
       "      <td>Valid email with a long local part</td>\n",
       "      <td>Pass</td>\n",
       "      <td>verylongusername1234567890@example.com</td>\n",
       "      <td>c1a3a7ff-83f4-46be-b207-2c15d2f188d2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rx Bc Demographics.Rx BC Email</td>\n",
       "      <td>TC003_Valid_MultipleDots</td>\n",
       "      <td>Valid email with multiple dots in local part</td>\n",
       "      <td>Pass</td>\n",
       "      <td>user.name.123@example.com</td>\n",
       "      <td>c2194d6f-2a23-4082-a85b-a89583f10c0c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rx Bc Demographics.Rx BC Email</td>\n",
       "      <td>TC004_Invalid_Null</td>\n",
       "      <td>Null input test</td>\n",
       "      <td>Fail</td>\n",
       "      <td>None</td>\n",
       "      <td>b6d56536-cec8-449b-8a7b-1a40a3e5dcf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rx Bc Demographics.Rx BC Email</td>\n",
       "      <td>TC005_Invalid_Empty</td>\n",
       "      <td>Empty string input test</td>\n",
       "      <td>Fail</td>\n",
       "      <td></td>\n",
       "      <td>68a1de47-95ef-4ece-b8c2-bac72cdbb124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Rx BC Email Event.Rx BC User Group</td>\n",
       "      <td>TC006_Invalid_WhitespaceOnly</td>\n",
       "      <td>Whitespace only input test</td>\n",
       "      <td>Fail</td>\n",
       "      <td></td>\n",
       "      <td>839b6025-dc2f-42da-b44a-02dcde1f10cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Rx BC Email Event.Rx BC User Group</td>\n",
       "      <td>TC007_Invalid_Number</td>\n",
       "      <td>Numeric input test</td>\n",
       "      <td>Fail</td>\n",
       "      <td>123</td>\n",
       "      <td>b96a4cb6-f12d-4974-8f3d-a29472a03c60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Rx BC Email Event.Rx BC User Group</td>\n",
       "      <td>TC008_Invalid_Boolean</td>\n",
       "      <td>Boolean input test</td>\n",
       "      <td>Fail</td>\n",
       "      <td>True</td>\n",
       "      <td>ef30bcf4-51f7-4338-9a7c-58c06c8b31af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Rx BC Email Event.Rx BC User Group</td>\n",
       "      <td>TC009_Edge_SingleChar</td>\n",
       "      <td>Single character input test</td>\n",
       "      <td>Pass</td>\n",
       "      <td>A</td>\n",
       "      <td>5082683e-f4e8-44ba-b074-b842225d1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Rx BC Email Event.Rx BC User Group</td>\n",
       "      <td>TC010_Edge_OnlyNumbers</td>\n",
       "      <td>Input containing only numbers</td>\n",
       "      <td>Pass</td>\n",
       "      <td>12345</td>\n",
       "      <td>919190f8-1422-40fd-9450-58069b068955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               category                     test_case  \\\n",
       "0        Rx Bc Demographics.Rx BC Email             TC001_Valid_Basic   \n",
       "1        Rx Bc Demographics.Rx BC Email         TC002_Valid_LongEmail   \n",
       "2        Rx Bc Demographics.Rx BC Email      TC003_Valid_MultipleDots   \n",
       "3        Rx Bc Demographics.Rx BC Email            TC004_Invalid_Null   \n",
       "4        Rx Bc Demographics.Rx BC Email           TC005_Invalid_Empty   \n",
       "..                                  ...                           ...   \n",
       "100  Rx BC Email Event.Rx BC User Group  TC006_Invalid_WhitespaceOnly   \n",
       "101  Rx BC Email Event.Rx BC User Group          TC007_Invalid_Number   \n",
       "102  Rx BC Email Event.Rx BC User Group         TC008_Invalid_Boolean   \n",
       "103  Rx BC Email Event.Rx BC User Group         TC009_Edge_SingleChar   \n",
       "104  Rx BC Email Event.Rx BC User Group        TC010_Edge_OnlyNumbers   \n",
       "\n",
       "                                      description expected_result  \\\n",
       "0                          Basic valid input test            Pass   \n",
       "1              Valid email with a long local part            Pass   \n",
       "2    Valid email with multiple dots in local part            Pass   \n",
       "3                                 Null input test            Fail   \n",
       "4                         Empty string input test            Fail   \n",
       "..                                            ...             ...   \n",
       "100                    Whitespace only input test            Fail   \n",
       "101                            Numeric input test            Fail   \n",
       "102                            Boolean input test            Fail   \n",
       "103                   Single character input test            Pass   \n",
       "104                 Input containing only numbers            Pass   \n",
       "\n",
       "                                      input  \\\n",
       "0                          test@example.com   \n",
       "1    verylongusername1234567890@example.com   \n",
       "2                 user.name.123@example.com   \n",
       "3                                      None   \n",
       "4                                             \n",
       "..                                      ...   \n",
       "100                                           \n",
       "101                                     123   \n",
       "102                                    True   \n",
       "103                                       A   \n",
       "104                                   12345   \n",
       "\n",
       "                                      key  \n",
       "0    3686623b-2261-4c79-8c10-6b6f8bc0824c  \n",
       "1    c1a3a7ff-83f4-46be-b207-2c15d2f188d2  \n",
       "2    c2194d6f-2a23-4082-a85b-a89583f10c0c  \n",
       "3    b6d56536-cec8-449b-8a7b-1a40a3e5dcf8  \n",
       "4    68a1de47-95ef-4ece-b8c2-bac72cdbb124  \n",
       "..                                    ...  \n",
       "100  839b6025-dc2f-42da-b44a-02dcde1f10cf  \n",
       "101  b96a4cb6-f12d-4974-8f3d-a29472a03c60  \n",
       "102  ef30bcf4-51f7-4338-9a7c-58c06c8b31af  \n",
       "103  5082683e-f4e8-44ba-b074-b842225d1239  \n",
       "104  919190f8-1422-40fd-9450-58069b068955  \n",
       "\n",
       "[105 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## laude with open ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "import openai\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('test_generation.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self, config_path: str = \"config/settings.yaml\"):\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.llm_client = self._initialize_llm()\n",
    "        self.field_specific_rules = self._initialize_field_rules()\n",
    "        \n",
    "    def _load_config(self, config_path: str) -> dict:\n",
    "        \"\"\"Load configuration from YAML file with error handling.\"\"\"\n",
    "        try:\n",
    "            with open(config_path, \"r\") as f:\n",
    "                return yaml.safe_load(f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load config: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_llm(self) -> openai.AzureOpenAI:\n",
    "        \"\"\"Initialize the OpenAI client with error handling.\"\"\"\n",
    "        try:\n",
    "            # Get Azure credentials\n",
    "            default_credential = DefaultAzureCredential()\n",
    "            access_token = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "            \n",
    "            if not access_token:\n",
    "                raise ValueError(\"Failed to obtain Azure access token\")\n",
    "            \n",
    "            # Initialize OpenAI client with Azure configuration\n",
    "            return openai.AzureOpenAI(\n",
    "                api_version=self.config.get(\"openai_api_version\", \"2024-06-01\"),\n",
    "                azure_endpoint=self.config.get(\"azure_openai_endpoint\", \"https://prod-1.services.unitedaistudio.uhg.com/aoai-shared-openai-prod-1\"),\n",
    "                api_key=access_token.token,\n",
    "                azure_deployment=self.config.get(\"deployment_name\", \"gpt-4o_2024-05-13\"),\n",
    "                default_headers={\n",
    "                    \"projectId\": self.config.get(\"project_id\", \"0bef8880-4e98-413c-bc0b-41c280fd1b2a\")\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize OpenAI client: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_field_rules(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Initialize specific rules for different field types.\"\"\"\n",
    "        return {\n",
    "            \"Date\": {\n",
    "                \"valid_formats\": [\n",
    "                    \"%Y-%m-%d %H:%M:%S\",\n",
    "                    \"%Y/%m/%d %H:%M:%S\",\n",
    "                    \"%m/%d/%Y %H:%M:%S\"\n",
    "                ],\n",
    "                \"extra_validation\": self._validate_date_format\n",
    "            },\n",
    "            \"String\": {\n",
    "                \"extra_validation\": self._validate_string_format\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _validate_date_format(self, test_case: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate date format test cases.\"\"\"\n",
    "        if test_case[\"input\"] is None:\n",
    "            return True, \"\"\n",
    "            \n",
    "        if isinstance(test_case[\"input\"], str):\n",
    "            for date_format in self.field_specific_rules[\"Date\"][\"valid_formats\"]:\n",
    "                try:\n",
    "                    datetime.strptime(test_case[\"input\"], date_format)\n",
    "                    return True, \"\"\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            return False, f\"Invalid date format. Expected formats: {self.field_specific_rules['Date']['valid_formats']}\"\n",
    "        return False, \"Date input must be a string\"\n",
    "\n",
    "    def _validate_string_format(self, test_case: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate string format test cases.\"\"\"\n",
    "        if test_case[\"input\"] is None:\n",
    "            return True, \"\"\n",
    "            \n",
    "        if not isinstance(test_case[\"input\"], (str, type(None))):\n",
    "            if test_case[\"expected_result\"] == \"Pass\":\n",
    "                return False, \"String field with non-string input should fail\"\n",
    "        return True, \"\"\n",
    "\n",
    "    def _generate_prompt(self, field_name: str, data_type: str, constraints: List[str], description: str = \"\") -> List[Dict[str, str]]:\n",
    "        \"\"\"Generate a more structured and specific prompt for test case generation.\"\"\"\n",
    "        field_specific_info = \"\"\n",
    "        if data_type == \"Date\":\n",
    "            field_specific_info = \"\\nFor Date fields, use these formats only:\\n\" + \\\n",
    "                                \"\\n\".join(f\"- {fmt}\" for fmt in self.field_specific_rules[\"Date\"][\"valid_formats\"])\n",
    "\n",
    "        prompt_content = f\"\"\"\n",
    "Generate test cases for the field '{field_name}' with following specifications:\n",
    "- Data Type: {data_type}\n",
    "- Constraints: {constraints}\n",
    "- Description: {description}{field_specific_info}\n",
    "\n",
    "Requirements:\n",
    "1. Include ONLY the JSON array of test cases in your response\n",
    "2. Each test case must have these exact fields:\n",
    "   - \"test_case\": A clear, unique identifier for the test\n",
    "   - \"description\": Detailed explanation of what the test verifies\n",
    "   - \"expected_result\": MUST be exactly \"Pass\" or \"Fail\"\n",
    "   - \"input\": The test input value (can be null, string, number, etc.)\n",
    "\n",
    "3. Include these types of test cases:\n",
    "   - Basic valid inputs\n",
    "   - Basic invalid inputs\n",
    "   - Null/empty handling\n",
    "   - Boundary conditions\n",
    "   - Edge cases\n",
    "   - Type validation\n",
    "\n",
    "4. Consider field-specific requirements:\n",
    "   - For Date fields: Include only valid date formats specified\n",
    "   - For String fields: Consider length limits and character restrictions\n",
    "   - Handle nullable fields appropriately based on constraints\n",
    "\n",
    "Return the response in this exact format:\n",
    "[\n",
    "    {{\n",
    "        \"test_case\": \"TC001_Valid_Basic\",\n",
    "        \"description\": \"Basic valid input test\",\n",
    "        \"expected_result\": \"Pass\",\n",
    "        \"input\": \"example\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "IMPORTANT: Return ONLY the JSON array. No additional text or explanation.\"\"\"\n",
    "\n",
    "        return [{\"role\": \"user\", \"content\": prompt_content}]\n",
    "\n",
    "    def _validate_test_case(self, test_case: Dict[str, Any], data_type: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate a single test case based on field type and rules.\"\"\"\n",
    "        if not all(field in test_case for field in [\"test_case\", \"description\", \"expected_result\", \"input\"]):\n",
    "            return False, \"Missing required fields\"\n",
    "\n",
    "        if test_case[\"expected_result\"] not in [\"Pass\", \"Fail\"]:\n",
    "            return False, \"Invalid expected_result value\"\n",
    "\n",
    "        # Apply field-specific validation\n",
    "        if data_type in self.field_specific_rules:\n",
    "            return self.field_specific_rules[data_type][\"extra_validation\"](test_case)\n",
    "\n",
    "        return True, \"\"\n",
    "\n",
    "    def _parse_llm_response(self, response_text: str, data_type: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        \"\"\"Parse and validate LLM response with improved error handling.\"\"\"\n",
    "        try:\n",
    "            # Remove Markdown JSON blocks if present\n",
    "            cleaned_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "            # Handle invalid escape sequences\n",
    "            cleaned_text = re.sub(r'\\\\([^\"\\\\])', r'\\\\\\\\\\1', cleaned_text)  \n",
    "\n",
    "            # Parse JSON\n",
    "            test_cases = json.loads(cleaned_text)\n",
    "\n",
    "            # Validate structure\n",
    "            if not isinstance(test_cases, list):\n",
    "                raise ValueError(\"Response is not a JSON array\")\n",
    "\n",
    "            # Validate and normalize each test case\n",
    "            validated_cases = []\n",
    "            for idx, case in enumerate(test_cases, 1):\n",
    "                is_valid, error_msg = self._validate_test_case(case, data_type)\n",
    "                if not is_valid:\n",
    "                    logging.warning(f\"Test case {idx} validation failed: {error_msg}\")\n",
    "                    continue\n",
    "                \n",
    "                # Normalize expected_result to Pass/Fail\n",
    "                case[\"expected_result\"] = \"Pass\" if case[\"expected_result\"].lower() == \"pass\" else \"Fail\"\n",
    "                validated_cases.append(case)\n",
    "\n",
    "            return validated_cases\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"JSON parsing error: {str(e)} - Raw response: {response_text}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error parsing response: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def generate_test_cases(self, rules_file: str, output_file: str) -> None:\n",
    "        \"\"\"Main method to generate and save test cases.\"\"\"\n",
    "        try:\n",
    "            # Load rules\n",
    "            with open(rules_file, \"r\") as f:\n",
    "                rules = json.load(f)\n",
    "\n",
    "            all_test_cases = {}\n",
    "            total_fields = sum(len(details[\"fields\"]) for details in rules.values())\n",
    "            processed_fields = 0\n",
    "\n",
    "            for parent_field, details in rules.items():\n",
    "                for field_name, field_details in details[\"fields\"].items():\n",
    "                    full_field_name = f\"{parent_field}.{field_name}\"\n",
    "                    logging.info(f\"Processing field {processed_fields + 1}/{total_fields}: {full_field_name}\")\n",
    "                    \n",
    "                    if full_field_name in all_test_cases:\n",
    "                        logging.warning(f\"Skipping {full_field_name}, already processed.\")\n",
    "                        continue\n",
    "\n",
    "                    # Generate prompt\n",
    "                    messages = self._generate_prompt(\n",
    "                        field_name,\n",
    "                        field_details[\"data_type\"],\n",
    "                        field_details[\"constraints\"],\n",
    "                        field_details.get(\"description\", \"\")\n",
    "                    )\n",
    "\n",
    "                    # Get OpenAI response with retries\n",
    "                    max_retries = 3\n",
    "                    for attempt in range(max_retries):\n",
    "                        try:\n",
    "                            response = self.llm_client.chat.completions.create(\n",
    "                                model=self.config.get(\"model_name\", \"gpt-4o\"),\n",
    "                                messages=messages,\n",
    "                            )\n",
    "                            \n",
    "                            test_cases = self._parse_llm_response(\n",
    "                                response.choices[0].message.content,\n",
    "                                field_details[\"data_type\"]\n",
    "                            )\n",
    "                            \n",
    "                            if test_cases:\n",
    "                                all_test_cases[full_field_name] = test_cases\n",
    "                                logging.info(f\"Successfully generated {len(test_cases)} test cases\")\n",
    "                                break\n",
    "                            else:\n",
    "                                logging.warning(f\"Attempt {attempt + 1}: Failed to generate valid test cases\")\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                            if attempt == max_retries - 1:\n",
    "                                logging.error(f\"Failed to generate test cases for {full_field_name} after {max_retries} attempts\")\n",
    "                    \n",
    "                    processed_fields += 1\n",
    "\n",
    "            # Save results\n",
    "            self._save_test_cases(all_test_cases, output_file)\n",
    "            \n",
    "            # Generate summary\n",
    "            self._generate_summary(all_test_cases, output_file)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate test cases: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _save_test_cases(self, test_cases: Dict[str, List[Dict[str, Any]]], output_file: str) -> None:\n",
    "        \"\"\"Save test cases with backup.\"\"\"\n",
    "        try:\n",
    "            # Create backup of existing file if it exists\n",
    "            if os.path.exists(output_file):\n",
    "                backup_file = f\"{output_file}.{datetime.now().strftime('%Y%m%d_%H%M%S')}.bak\"\n",
    "                os.rename(output_file, backup_file)\n",
    "                logging.info(f\"Created backup: {backup_file}\")\n",
    "\n",
    "            # Save new test cases\n",
    "            with open(output_file, \"w\") as f:\n",
    "                json.dump(test_cases, f, indent=2)\n",
    "            logging.info(f\"Successfully saved test cases to {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save test cases: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _generate_summary(self, test_cases: Dict[str, List[Dict[str, Any]]], output_file: str) -> None:\n",
    "        \"\"\"Generate a summary of the test case generation.\"\"\"\n",
    "        total_fields = len(test_cases)\n",
    "        total_test_cases = sum(len(cases) for cases in test_cases.values())\n",
    "        \n",
    "        summary = (\n",
    "            f\"\\nTest Case Generation Summary\\n\"\n",
    "            f\"{'='*30}\\n\"\n",
    "            f\"Total fields processed: {total_fields}\\n\"\n",
    "            f\"Total test cases generated: {total_test_cases}\\n\"\n",
    "            f\"Average test cases per field: {total_test_cases/total_fields:.2f}\\n\"\n",
    "            f\"Output file: {output_file}\\n\"\n",
    "            f\"{'='*30}\"\n",
    "        )\n",
    "        \n",
    "        logging.info(summary)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        generator = TestCaseGenerator()\n",
    "        generator.generate_test_cases(\n",
    "            generator.config[\"constrains_processed_rules_file\"],\n",
    "            generator.config[\"generated_test_cases_file\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Application failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "import openai\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('test_generation.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self, config_path: str = \"config/settings.yaml\"):\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.llm_client = self._initialize_llm()\n",
    "        self.field_specific_rules = self._initialize_field_rules()\n",
    "        \n",
    "    def _load_config(self, config_path: str) -> dict:\n",
    "        \"\"\"Load configuration from YAML file with error handling.\"\"\"\n",
    "        try:\n",
    "            with open(config_path, \"r\") as f:\n",
    "                return yaml.safe_load(f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load config: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_llm(self):\n",
    "        \"\"\"Initialize the OpenAI LLM client using Azure authentication.\"\"\"\n",
    "        try:\n",
    "            credential = DefaultAzureCredential()\n",
    "            access_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "            \n",
    "            api_key = access_token\n",
    "            os.environ[\"api_key\"] = api_key\n",
    "            os.environ[\"AZURE_OPENAI_API_KEY\"] = api_key\n",
    "            \n",
    "            return openai.AzureOpenAI(\n",
    "                api_version=self.config.get(\"openai_api_version\", \"2024-06-01\"),\n",
    "                azure_endpoint=self.config.get(\"azure_openai_endpoint\"),\n",
    "                api_key=api_key,\n",
    "                azure_deployment=self.config.get(\"deployment_name\", \"gpt-4o_2024-05-13\"),\n",
    "                default_headers={\"projectId\": self.config.get(\"project_id\", \"\")}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize OpenAI LLM: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _generate_prompt(self, field_name: str, data_type: str, constraints: List[str], description: str = \"\") -> str:\n",
    "        \"\"\"Generate a structured prompt for test case generation.\"\"\"\n",
    "        return f\"\"\"\n",
    "Generate test cases for the field '{field_name}' with following specifications:\n",
    "- Data Type: {data_type}\n",
    "- Constraints: {constraints}\n",
    "- Description: {description}\n",
    "\n",
    "Return ONLY a JSON array with fields: \"test_case\", \"description\", \"expected_result\", \"input\".\"\"\"\n",
    "\n",
    "    def generate_test_cases(self, rules_file: str, output_file: str) -> None:\n",
    "        \"\"\"Main method to generate and save test cases.\"\"\"\n",
    "        try:\n",
    "            with open(rules_file, \"r\") as f:\n",
    "                rules = json.load(f)\n",
    "\n",
    "            all_test_cases = {}\n",
    "            for parent_field, details in rules.items():\n",
    "                for field_name, field_details in details[\"fields\"].items():\n",
    "                    prompt = self._generate_prompt(\n",
    "                        field_name,\n",
    "                        field_details[\"data_type\"],\n",
    "                        field_details[\"constraints\"],\n",
    "                        field_details.get(\"description\", \"\")\n",
    "                    )\n",
    "                    \n",
    "                    response = self.llm_client.chat.completions.create(\n",
    "                        model=\"gpt-4o\",\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                    )\n",
    "                    \n",
    "                    test_cases = json.loads(response.choices[0].message.content)\n",
    "                    all_test_cases[field_name] = test_cases\n",
    "                    logging.info(f\"Generated {len(test_cases)} test cases for {field_name}\")\n",
    "\n",
    "            with open(output_file, \"w\") as f:\n",
    "                json.dump(all_test_cases, f, indent=2)\n",
    "            logging.info(f\"Successfully saved test cases to {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate test cases: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        generator = TestCaseGenerator()\n",
    "        generator.generate_test_cases(\n",
    "            generator.config[\"constrains_processed_rules_file\"],\n",
    "            generator.config[\"generated_test_cases_file\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Application failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
